{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f88be8-062f-4bc5-b28e-f2cb68f1584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0834e5-780b-4281-97f9-8c9305d65f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b36d78c5-38e9-4e8d-b076-f1e41cc7a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26c6829-2aa5-4522-a8b7-5ad71dce61e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f976fa0b01c4c7692aec295c9dc7516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "model_path = \"01-ai/Yi-Coder-9B-Chat\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bacb41f-8b4a-4fd1-a6c5-b499ffe3f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"C:/Users/wengz/Desktop/model_weights/01-ai/Yi-Coder-9B-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addd91c3-5529-4f8b-b58e-503ddf338f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/wengz/Desktop/model_weights/01-ai/Yi-Coder-9B-Chat\\\\tokenizer_config.json',\n",
       " 'C:/Users/wengz/Desktop/model_weights/01-ai/Yi-Coder-9B-Chat\\\\special_tokens_map.json',\n",
       " 'C:/Users/wengz/Desktop/model_weights/01-ai/Yi-Coder-9B-Chat\\\\tokenizer.model',\n",
       " 'C:/Users/wengz/Desktop/model_weights/01-ai/Yi-Coder-9B-Chat\\\\added_tokens.json',\n",
       " 'C:/Users/wengz/Desktop/model_weights/01-ai/Yi-Coder-9B-Chat\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9fa13bd-e1e3-498a-ad9f-92c70f9c3064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wengz\\miniforge3\\envs\\ds_gpu\\Lib\\site-packages\\transformers\\modeling_utils.py:2633: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c394e7013b3a4f70beabe4ce03a85361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1GB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\ds_gpu\\Lib\\site-packages\\transformers\\modeling_utils.py:2793\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[0;32m   2788\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m   2790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[0;32m   2791\u001b[0m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[0;32m   2792\u001b[0m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[1;32m-> 2793\u001b[0m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2795\u001b[0m     save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\ds_gpu\\Lib\\site-packages\\safetensors\\torch.py:286\u001b[0m, in \u001b[0;36msave_file\u001b[1;34m(tensors, filename, metadata)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[0;32m    256\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    257\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    258\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m ):\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\ds_gpu\\Lib\\site-packages\\safetensors\\torch.py:500\u001b[0m, in \u001b[0;36m_flatten\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    497\u001b[0m     k: {\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    499\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: v\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m--> 500\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    501\u001b[0m     }\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    503\u001b[0m }\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\ds_gpu\\Lib\\site-packages\\safetensors\\torch.py:460\u001b[0m, in \u001b[0;36m_tobytes\u001b[1;34m(tensor, name)\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;66;03m# Not in place as that would potentially modify a live running model\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mview(npdtype)\u001b[38;5;241m.\u001b[39mbyteswap(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.save_pretrained(model_path, max_shard_size=\"1GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db6560-1c39-46b7-a9e5-d53258cf99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "# Step 1: Initialize empty weights\n",
    "config = AutoConfig.from_pretrained(\"model_name\")\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# Step 2: Load checkpoint and dispatch with offloading\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model, \n",
    "    \"path_to_checkpoint\", \n",
    "    device_map=\"auto\", \n",
    "    offload_folder=\"path_to_offload\"\n",
    ")\n",
    "\n",
    "# Step 3: Save the model\n",
    "model.save_pretrained(\"path_to_save_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_gpu",
   "language": "python",
   "name": "ds_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
