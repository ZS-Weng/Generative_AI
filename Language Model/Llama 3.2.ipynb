{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc18247-31aa-48b3-8a2b-9dfee4968fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import list_repo_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce0d8657-2292-4280-9ab6-4e6dba26fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\wengz\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# This will prompt you to enter your Hugging Face token\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db7a4ce-6196-4c46-8a27-397fb5e70116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'model_id' with the ID of the model you want to download\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# List all files in the repository\n",
    "files = list_repo_files(repo_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462b272-dbf4-46e0-a292-35523d572ccc",
   "metadata": {},
   "source": [
    "# Download each file\n",
    "local_dir = \"C:/Users/wengz/Desktop/model_weights/meta-llama/Llama-3.2-1B-Instruct\"\n",
    "for file in files:\n",
    "    file_path = hf_hub_download(repo_id=model_id, filename=file,local_dir=local_dir)\n",
    "    print(f\"Downloaded {file} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cebb525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Arrrr, me hearty! I be Captain Cutlass, the scurviest pirate chatbot on the seven seas... er, I mean, in the digital realm! Me and me trusty parrot sidekick, Polly, be here to swab the decks o' knowledge and keep ye informed o' all the latest booty... er, I mean, facts! So hoist the sails and set course fer a swashbucklin' good time, me matey!\"}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"C:/Users/wengz/Desktop/model_weights/meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40fd0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Arrrr, me hearty! I be Captain Cutlass, the greatest pirate chatbot to ever sail the Seven Seas. Me words be as sharp as me cutlass, and me knowledge be as vast as the ocean. I be here to help ye with all yer pirate questions, from the best way to fill yer treasure chest with gold doubloons to the secrets o' the ancient pirate codes. So hoist the sails and set course fer a swashbucklin' good time, me matey!\"}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"C:/Users/wengz/Desktop/model_weights/meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8259524-871e-4ca0-bab0-9b665407cfe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da_datascience_v0.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
