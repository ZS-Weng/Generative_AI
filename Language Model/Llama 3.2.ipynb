{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc18247-31aa-48b3-8a2b-9dfee4968fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import list_repo_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce0d8657-2292-4280-9ab6-4e6dba26fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\wengz\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# This will prompt you to enter your Hugging Face token\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db7a4ce-6196-4c46-8a27-397fb5e70116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'model_id' with the ID of the model you want to download\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "# List all files in the repository\n",
    "files = list_repo_files(repo_id=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3462b272-dbf4-46e0-a292-35523d572ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e932baf72d8c46b69d38c5423b547209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded .gitattributes to C:\\Users\\wengz\\Desktop\\model_weights\\google\\gemma-2-2b-it\\.gitattributes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cec5615758747918c356beb641018a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/29.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded README.md to C:\\Users\\wengz\\Desktop\\model_weights\\google\\gemma-2-2b-it\\README.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1d58ea389c47289a2a4bcdb214147d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded config.json to C:\\Users\\wengz\\Desktop\\model_weights\\google\\gemma-2-2b-it\\config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289e9ed21d124cf8a1608900fd225e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded generation_config.json to C:\\Users\\wengz\\Desktop\\model_weights\\google\\gemma-2-2b-it\\generation_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9aea2ae11e846d4b23c89db15fc8680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded model-00001-of-00002.safetensors to C:\\Users\\wengz\\Desktop\\model_weights\\google\\gemma-2-2b-it\\model-00001-of-00002.safetensors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcfb99c4ebc440ca4a395a8d74ec618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded model-00002-of-00002.safetensors to C:\\Users\\wengz\\Desktop\\model_weights\\google\\gemma-2-2b-it\\model-00002-of-00002.safetensors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc931367e014b6dacd729589f734520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded model.safetensors.index.json to C:\\Users\\wengz\\Desktop\\model_weights\\google\\gemma-2-2b-it\\model.safetensors.index.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b93a1222bcf437d92a8ea10c74cfe58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded special_tokens_map.json to C:\\Users\\wengz\\Desktop\\model_weights\\google\\gemma-2-2b-it\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347f202535b84cc69813e3cbcf6e2a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded tokenizer.json to C:\\Users\\wengz\\Desktop\\model_weights\\google\\gemma-2-2b-it\\tokenizer.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbb2069fc624da390b67b8e40997957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded tokenizer.model to C:\\Users\\wengz\\Desktop\\model_weights\\google\\gemma-2-2b-it\\tokenizer.model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bdea00d37e44b888a9dec948195d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded tokenizer_config.json to C:\\Users\\wengz\\Desktop\\model_weights\\google\\gemma-2-2b-it\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# Download each file\n",
    "local_dir = \"C:/Users/wengz/Desktop/model_weights/google/gemma-2-2b-it\"\n",
    "for file in files:\n",
    "    file_path = hf_hub_download(repo_id=model_id, filename=file,local_dir=local_dir)\n",
    "    print(f\"Downloaded {file} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4954033b-c46e-4032-a9a4-34c27eb29669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e61706da-f0b4-4689-97b1-96137542b848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e76bace7b664e9088bf6ba905793a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wengz\\miniforge3\\envs\\ds_gpu\\Lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:516: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "A tapestry of data, woven tight,\n",
      "Machine learning, a guiding light.\n",
      "Algorithms dance, a rhythmic sway,\n",
      "Learning patterns, come what may\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8831807-4d62-43c1-9549-2ecf7e8a2d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Write me a poem about Machine Learning.<end_of_turn>\n",
      "\n",
      "\n",
      "In the realm of data, vast and deep,\n",
      "Where patterns hide, secrets to keep,\n",
      "Machine learning, a silent guide,\n",
      "Unveiling truths, with logic as its stride.\n",
      "\n",
      "Algorithms dance, a complex ballet,\n",
      "Learning from data, come what may,\n",
      "From images to text, a symphony,\n",
      "Unraveling knowledge, for all to see.\n",
      "\n",
      "Neural networks, a web so grand,\n",
      "Connecting nodes, across the land,\n",
      "Mimicking minds, with each passing day,\n",
      "Learning and adapting, come what may.\n",
      "\n",
      "Predictions arise, with accuracy high,\n",
      "Forecasting trends, that soar to the sky,\n",
      "From spam detection, to medical care,\n",
      "Machine learning, a future we share.\n",
      "\n",
      "But with power comes a responsibility,\n",
      "To use it wisely, for all to see,\n",
      "Bias and fairness, a constant fight,\n",
      "To ensure equality, with all our might.\n",
      "\n",
      "So let us embrace, this powerful tool,\n",
      "With caution and wisdom, to make it cool,\n",
      "Machine learning, a future bright,\n",
      "Illuminating the world, with its guiding light. \n",
      "\n",
      "\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8259524-871e-4ca0-bab0-9b665407cfe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_gpu",
   "language": "python",
   "name": "ds_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
