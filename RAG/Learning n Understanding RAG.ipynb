{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d13502f",
   "metadata": {},
   "source": [
    "## Notebook on learning about RAG\n",
    "\n",
    "- Good resource: https://learnbybuilding.ai/tutorials/rag-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ea8d3",
   "metadata": {},
   "source": [
    "### Benefits of RAG written in the tutorial\n",
    "\n",
    "- You can include facts in the prompt to help the LLM avoid hallucinations\n",
    "- You can (manually) refer to sources of truth when responding to a user query, helping to double check any potential issues.\n",
    "- You can leverage data that the LLM might not have been trained on.\n",
    "\n",
    "### The High Level Components of our RAG System\n",
    "- a collection of documents (formally called a corpus)\n",
    "- An input from the user\n",
    "- a similarity measure between the collection of documents and the user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e02811",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_of_documents = [\n",
    "    \"Take a leisurely walk in the park and enjoy the fresh air.\",\n",
    "    \"Visit a local museum and discover something new.\",\n",
    "    \"Attend a live music concert and feel the rhythm.\",\n",
    "    \"Go for a hike and admire the natural scenery.\",\n",
    "    \"Have a picnic with friends and share some laughs.\",\n",
    "    \"Explore a new cuisine by dining at an ethnic restaurant.\",\n",
    "    \"Take a yoga class and stretch your body and mind.\",\n",
    "    \"Join a local sports league and enjoy some friendly competition.\",\n",
    "    \"Attend a workshop or lecture on a topic you're interested in.\",\n",
    "    \"Visit an amusement park and ride the roller coasters.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce4c8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    query = query.lower().split(\" \")\n",
    "    document = document.lower().split(\" \")\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619aa1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_response(query, corpus):\n",
    "    similarities = []\n",
    "    for doc in corpus:\n",
    "        similarity = jaccard_similarity(user_input, doc)\n",
    "        similarities.append(similarity)\n",
    "    return corpus_of_documents[similarities.index(max(similarities))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cedb14f",
   "metadata": {},
   "source": [
    "## Understanding in the lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654a224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How should I take a walk ?\"\n",
    "document = corpus_of_documents[0]\n",
    "query = query.lower().split(\" \")\n",
    "document = document.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86c699e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 'walk', 'take'} \n",
      " {'should', 'i', 'and', 'a', 'enjoy', 'leisurely', 'the', 'walk', 'take', 'park', 'air.', '?', 'in', 'how', 'fresh'}\n"
     ]
    }
   ],
   "source": [
    "intersection = set(query).intersection(set(document))\n",
    "union = set(query).union(set(document))\n",
    "print(intersection, \"\\n\", union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6419f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How should I take a walk ?\"\n",
    "corpus = corpus_of_documents\n",
    "similarities = []\n",
    "for doc in corpus:\n",
    "    similarity = jaccard_similarity(query, doc)\n",
    "    similarities.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88710dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8efb8c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities.index(max(similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5571b2",
   "metadata": {},
   "source": [
    "## Implemenation of an Open Source Semantic Search model\n",
    "\n",
    "- Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b8e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63cdff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # imports the pymupdf library\n",
    "doc = fitz.open(\"2022ltr.pdf\") # open a document\n",
    "pdf_texts =  [page.get_text() for page in doc] # iterate the document pages\n",
    "pdf_texts = [text for text in pdf_texts if len(text)>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80017f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERKSHIRE HATHAWAY INC.\\nTo the Shareholders of Berkshire Hathaway Inc.:\\nCharlie Munger, my long-time partner, and I have the job of managing the savings of a\\ngreat number of individuals. We are grateful for their enduring trust, a relationship that often spans\\nmuch of their adult lifetime. It is those dedicated savers that are forefront in my mind as I write\\nthis letter.\\nA common belief is that people choose to save when young, expecting thereby to maintain\\ntheir living standards after retirement. Any assets that remain at death, this theory says, will usually\\nbe left to their families or, possibly, to friends and philanthropy.\\nOur experience has differed. We believe Berkshire’s individual holders largely to be of the\\nonce-a-saver, always-a-saver variety. Though these people live well, they eventually dispense\\nmost of their funds to philanthropic organizations. These, in turn, redistribute the funds by\\nexpenditures intended to improve the lives of a great many people who are unrelated to the original\\nbenefactor. Sometimes, the results have been spectacular.\\nThe disposition of money unmasks humans. Charlie and I watch with pleasure the vast flow\\nof Berkshire-generated funds to public needs and, alongside, the infrequency with which our\\nshareholders opt for look-at-me assets and dynasty-building.\\nWho wouldn’t enjoy working for shareholders like ours?\\nWhat We Do\\nCharlie and I allocate your savings at Berkshire between two related forms of ownership.\\nFirst, we invest in businesses that we control, usually buying 100% of each. Berkshire directs\\ncapital allocation at these subsidiaries and selects the CEOs who make day-by-day operating\\ndecisions. When large enterprises are being managed, both trust and rules are essential. Berkshire\\nemphasizes the former to an unusual – some would say extreme – degree. Disappointments are\\ninevitable. We are understanding about business mistakes; our tolerance for personal misconduct\\nis zero.\\nIn our second category of ownership, we buy publicly-traded stocks through which we\\npassively own pieces of businesses. Holding these investments, we have no say in management.\\n3\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5740bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9315ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do the math: See’s rang up about 10 sales per minute during its prime operating time\n",
      "(racking up $400,309 of volume during the two days), with all the goods purchased at a single\n",
      "location selling products that haven’t been materially altered in 101 years. What worked for See’s\n",
      "in the days of Henry Ford’s model T works now.\n",
      "* * * * * * * * * * * *\n",
      "Charlie, I, and the entire Berkshire bunch look forward to seeing you in Omaha on\n",
      "May 5-6. We will have a good time and so will you.\n",
      "February 25, 2023\n",
      "Warren E. Buffett\n",
      "Chairman of the Board\n",
      "11\n",
      "\n",
      "Total chunks: 36\n"
     ]
    }
   ],
   "source": [
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
    "\n",
    "print(character_split_texts[35])\n",
    "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3407a6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wengz\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do the math : see ’ s rang up about 10 sales per minute during its prime operating time ( racking up $ 400, 309 of volume during the two days ), with all the goods purchased at a single location selling products that haven ’ t been materially altered in 101 years. what worked for see ’ s in the days of henry ford ’ s model t works now. * * * * * * * * * * * * charlie, i, and the entire berkshire bunch look forward to seeing you in omaha on may 5 - 6. we will have a good time and so will you. february 25, 2023 warren e. buffett chairman of the board 11\n",
      "\n",
      "Total chunks: 46\n"
     ]
    }
   ],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "\n",
    "token_split_texts = []\n",
    "for text in character_split_texts:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(token_split_texts[45])\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e66068e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.012467734515666962, -0.05428552255034447, 0.037649743258953094, -0.02378346025943756, 0.011054451577365398, 0.02136710099875927, -0.06601178646087646, -0.002584449015557766, 0.01125031616538763, -0.026465097442269325, 0.021672572940587997, 0.08212035149335861, -0.02570822462439537, -0.05478880926966667, -0.019596856087446213, -0.026295682415366173, 0.08333292603492737, -0.06668368726968765, -0.003572107758373022, -0.06448515504598618, -0.02125905081629753, -0.0254600141197443, -0.05526052787899971, 0.026272263377904892, -0.01911891996860504, 0.009858915582299232, -0.015476008877158165, -0.0454326868057251, -0.027211172506213188, -0.06809180229902267, -0.10900609940290451, -0.0048833186738193035, -0.041496992111206055, 0.01615954376757145, 0.013104509562253952, -0.04445979371666908, 0.06840939819812775, -0.060902856290340424, 0.04529665783047676, -0.05040785297751427, 0.07744777202606201, -0.04709912836551666, -0.04672233387827873, -0.017751457169651985, 0.07489844411611557, 0.016270417720079422, 0.04388357698917389, 0.11512205004692078, 0.04081427678465843, 0.01337850745767355, -0.09153995662927628, 0.017361653968691826, -0.044410426169633865, -0.08701522648334503, -0.05975135788321495, 0.10621161758899689, 0.00823866855353117, -0.027469288557767868, 0.006793755106627941, 0.0025355236139148474, 0.033523011952638626, -0.0918697789311409, 0.0324038602411747, 0.009494616650044918, 0.03698417544364929, 0.02920786663889885, -0.08006119728088379, -0.022133775055408478, -0.07310070097446442, 0.031024377793073654, 0.006625120062381029, -0.0067049176432192326, -0.0139502277597785, -0.008831603452563286, -0.03570624813437462, -0.0592883862555027, 0.07602732628583908, -0.022726934403181076, 0.04920072481036186, -0.025856900960206985, -0.035155389457941055, -0.037047307938337326, -0.055110737681388855, -0.04341666400432587, -0.0032733588013798, -0.04491177573800087, 0.024930011481046677, 0.04584571346640587, 0.07261548191308975, -0.057157691568136215, -0.012116988189518452, -0.02322476916015148, -0.052646465599536896, -0.018416590988636017, -0.13621439039707184, -0.015065073035657406, 0.0015929985092952847, 0.0968254804611206, -0.030156061053276062, 0.0330643393099308, 0.020262038335204124, 0.127543106675148, 0.022033916786313057, -0.08231010288000107, -0.1279505044221878, -0.012395595200359821, 0.00015406716556753963, 0.1187783032655716, 0.027363847941160202, 0.02710680104792118, -0.061389271169900894, 0.02898043394088745, 0.02255774661898613, -0.061217404901981354, -0.05271302908658981, -0.004338503815233707, -0.0388883613049984, -0.06158735975623131, 0.05732757970690727, -0.007183299865573645, 0.1193302720785141, 0.1122124195098877, 0.05645042285323143, 0.08006945252418518, -0.0769575908780098, 0.03382766991853714, 0.013946662656962872, 1.9643225082400533e-33, -0.04063413292169571, -0.010819386690855026, 0.040554195642471313, -0.01648377627134323, -0.008976412937045097, 0.04253576323390007, -0.030332855880260468, -0.07849302142858505, -0.009044576436281204, 0.023378299549221992, 0.012828214094042778, -0.04674139246344566, -0.035647809505462646, 0.002722271019592881, 0.005559652578085661, -0.08468582481145859, 0.04115716367959976, 0.029436152428388596, 0.026771636679768562, -0.09659630805253983, 0.0023510728497058153, -0.04695645719766617, -0.07301563024520874, -0.0028912147972732782, 0.023459114134311676, -0.004399844910949469, -0.004540279041975737, 0.02817082777619362, 0.0130142942070961, 0.04565799608826637, 0.08517032861709595, 0.12059515714645386, -0.09447059035301208, -0.017398308962583542, -0.06602457165718079, -0.017837395891547203, -0.007903927005827427, -0.05357541888952255, 0.10234574228525162, -0.09594576805830002, -0.07608769834041595, 0.06854008138179779, 0.03387945890426636, -0.027054550126194954, -0.05555512383580208, 0.017269916832447052, 0.10273362696170807, 0.09309326112270355, 0.01815938577055931, 0.0013228956377133727, -0.08230724930763245, -0.015453102067112923, 0.04333340376615524, 0.011398511938750744, -0.010177157819271088, -0.034997425973415375, 0.0076638078317046165, -0.09278253465890884, 0.011377960443496704, 0.01624978706240654, 0.08639577776193619, 0.08101734519004822, 0.011446270160377026, 0.024012867361307144, -0.17709992825984955, 0.09735089540481567, 0.04299301281571388, 0.00935098621994257, -0.01044517569243908, 0.0925411581993103, 0.10087248682975769, 0.0049667758867144585, 0.03146492317318916, -0.08096455782651901, 0.03499830514192581, -0.04934541508555412, 0.05192093923687935, 0.003076043911278248, 0.08170250803232193, 0.05989013612270355, 0.053060829639434814, -0.05741119384765625, 0.036876849830150604, 0.019664356485009193, 0.06395161896944046, 0.039275623857975006, 0.025246288627386093, -0.013260708190500736, 0.0029740945901721716, -0.013389445841312408, 0.004467286169528961, -0.004224961157888174, 0.04421495646238327, 0.057754985988140106, -0.02399374544620514, -3.749753143384186e-33, -0.039862003177404404, 0.017725564539432526, 0.030118023976683617, -0.013622567057609558, -0.052312254905700684, -0.09232281893491745, -0.04267428442835808, 0.06041155755519867, 0.001489965827204287, -0.013101343996822834, -0.052496131509542465, 0.050226934254169464, -0.02263774536550045, -0.03254348784685135, -0.04090793803334236, -0.01848706603050232, 0.13529770076274872, -0.06063840538263321, 0.062453776597976685, -0.07041114568710327, 0.025574900209903717, 0.00683961296454072, -0.06749928742647171, 0.016969040036201477, -0.03756901994347572, 0.05369553714990616, 0.023542247712612152, 0.007032283116132021, -0.030139824375510216, -0.10239946097135544, -0.09089936316013336, -0.05848846584558487, -0.03048205003142357, 0.0686674565076828, -0.014937475323677063, 0.0004830970137845725, 0.014606175012886524, -0.021516280248761177, 0.012521914206445217, -0.05047817900776863, 0.06332079321146011, -0.017677435651421547, -0.011792323552072048, 0.017487429082393646, 0.02478894405066967, 0.046104248613119125, 0.02594153955578804, 0.030529042705893517, 0.06112968176603317, 0.09002084285020828, -0.027855947613716125, 0.02887093834578991, 0.024166064336895943, 0.03191343694925308, -0.08654240518808365, 0.0649852529168129, 0.045300889760255814, 0.01905837096273899, -0.040079835802316666, -0.011395602487027645, -0.0004823681083507836, 0.05134503170847893, 0.004446709994226694, 0.04563996568322182, 0.01732843741774559, -0.060062944889068604, 0.014314540661871433, -0.02247176505625248, 0.0436621755361557, 0.00013079823111183941, -0.0023668273352086544, -0.025252601131796837, -0.06394464522600174, -0.07349810749292374, -0.02121778577566147, 0.08652467280626297, 0.030526915565133095, 0.027590760961174965, 0.0002873373741749674, 0.020030619576573372, -0.06737039238214493, -0.006077103782445192, 0.029869528487324715, -0.023509422317147255, 0.030618775635957718, 0.02264999784529209, 0.006062669679522514, -0.019230015575885773, -0.004782724194228649, 0.0922505334019661, -0.06152433902025223, -0.02732999436557293, -0.0034875308629125357, 0.019230026751756668, -0.03886675462126732, -4.9227747211944006e-08, 0.06261337548494339, 0.005570422857999802, 0.07000184804201126, -0.03292569890618324, 0.07600996643304825, -0.08808532357215881, 0.040107954293489456, 0.037450991570949554, -0.016938095912337303, 0.009976672008633614, 0.012307576835155487, -0.01260110642760992, -0.024086108431220055, 0.006967518012970686, 0.022596577182412148, -0.02955503575503826, -0.032187268137931824, -0.03658805787563324, -0.024897251278162003, -0.06197062134742737, 0.0024513020180165768, 0.035185012966394424, 0.13533304631710052, 0.0021127841901034117, -0.04549028351902962, 0.06390628963708878, 0.008539990521967411, 0.0445159375667572, 0.010151383467018604, 0.011128980666399002, -0.04176098108291626, -0.007519041653722525, -0.02037615329027176, -0.025944871827960014, 0.00809503998607397, -0.0718160942196846, -0.13540847599506378, 0.045145392417907715, -0.01638454757630825, 0.011370145715773106, -0.036067672073841095, -0.04218999296426773, 0.007682261057198048, 0.07460904121398926, 0.08479833602905273, -0.020251717418432236, -0.0632578507065773, -0.014723489992320538, 0.006335030309855938, -0.05858225002884865, -0.012562839314341545, 0.01961504854261875, -0.01589898020029068, -0.03447437286376953, 0.0017600433202460408, -0.003507043235003948, 0.04166217893362045, -0.059219345450401306, -0.004755656234920025, 0.030771588906645775, 0.05344247445464134, -0.13006842136383057, -0.10022895038127899, 0.055306147783994675]]\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "print(embedding_function([token_split_texts[45]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fad0cf",
   "metadata": {},
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"Berkshire_Annual_Report\", embedding_function=embedding_function)\n",
    "\n",
    "ids = [str(i) for i in range(len(token_split_texts))]\n",
    "\n",
    "chroma_collection.add(ids=ids, documents=token_split_texts)\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d67f4106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=Berkshire_Annual_Report)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9434e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "chroma_collection = chroma_client.get_collection(\"Berkshire_Annual_Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12a8858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 4\n",
      "Add of existing embedding ID: 5\n",
      "Add of existing embedding ID: 6\n",
      "Add of existing embedding ID: 7\n",
      "Add of existing embedding ID: 8\n",
      "Add of existing embedding ID: 9\n",
      "Add of existing embedding ID: 10\n",
      "Add of existing embedding ID: 11\n",
      "Add of existing embedding ID: 12\n",
      "Add of existing embedding ID: 13\n",
      "Add of existing embedding ID: 14\n",
      "Add of existing embedding ID: 15\n",
      "Add of existing embedding ID: 16\n",
      "Add of existing embedding ID: 17\n",
      "Add of existing embedding ID: 18\n",
      "Add of existing embedding ID: 19\n",
      "Add of existing embedding ID: 20\n",
      "Add of existing embedding ID: 21\n",
      "Add of existing embedding ID: 22\n",
      "Add of existing embedding ID: 23\n",
      "Add of existing embedding ID: 24\n",
      "Add of existing embedding ID: 25\n",
      "Add of existing embedding ID: 26\n",
      "Add of existing embedding ID: 27\n",
      "Add of existing embedding ID: 28\n",
      "Add of existing embedding ID: 29\n",
      "Add of existing embedding ID: 30\n",
      "Add of existing embedding ID: 31\n",
      "Add of existing embedding ID: 32\n",
      "Add of existing embedding ID: 33\n",
      "Add of existing embedding ID: 34\n",
      "Add of existing embedding ID: 35\n",
      "Add of existing embedding ID: 36\n",
      "Add of existing embedding ID: 37\n",
      "Add of existing embedding ID: 38\n",
      "Add of existing embedding ID: 39\n",
      "Add of existing embedding ID: 40\n",
      "Add of existing embedding ID: 41\n",
      "Add of existing embedding ID: 42\n",
      "Add of existing embedding ID: 43\n",
      "Add of existing embedding ID: 44\n",
      "Add of existing embedding ID: 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• patience can be learned. having a long attention span and the ability to concentrate on one thing for a long time is a huge advantage. • you can learn a lot from dead people. read of the deceased you admire and detest. • don ’ t bail away in a sinking boat if you can swim to one that is seaworthy. • a great company keeps working after you are not ; a mediocre company won ’ t do that. • warren and i don ’ t focus on the froth of the market. we seek out good long - term investments and stubbornly hold them for a long time. • ben graham said, “ day to day, the stock market is a voting machine ; in the long term it ’ s a weighing machine. ” if you keep making something more valuable, then some wise person is going to notice it and start buying. • there is no such thing as a 100 % sure thing when investing. thus, the use of leverage is dangerous. a string of wonderful numbers times zero will always equal zero. don ’ t count on getting rich twice.\n",
      "\n",
      "\n",
      "• you don ’ t, however, need to own a lot of things in order to get rich. • you have to keep learning if you want to become a great investor. when the world changes, you must change. • warren and i hated railroad stocks for decades, but the world changed and finally the country had four huge railroads of vital importance to the american economy. we were slow to recognize the change, but better late than never. • finally, i will add two short sentences by charlie that have been his decision - clinchers for decades : “ warren, think more about it. you ’ re smart and i ’ m right. ” and so it goes. i never have a phone call with charlie without learning something. and, while he makes me think, he also makes me laugh. * * * * * * * * * * * * i will add to charlie ’ s list a rule of my own : find a very smart high - grade partner – preferably slightly older than you – and then listen very carefully to what he says. a family gathering in omaha\n",
      "\n",
      "\n",
      "inevitable. we are understanding about business mistakes ; our tolerance for personal misconduct is zero. in our second category of ownership, we buy publicly - traded stocks through which we passively own pieces of businesses. holding these investments, we have no say in management. 3\n",
      "\n",
      "\n",
      "delivering a gusher of improved goods and services. schumpeter called this phenomenon “ creative destruction. ” one advantage of our publicly - traded segment is that – episodically – it becomes easy to buy pieces of wonderful businesses at wonderful prices. it ’ s crucial to understand that stocks often trade at truly foolish prices, both high and low. “ efficient ” markets exist only in textbooks. in truth, marketable stocks and bonds are baffling, their behavior usually understandable only in retrospect. controlled businesses are a different breed. they sometimes command ridiculously higher prices than justified but are almost never available at bargain valuations. unless under duress, the owner of a controlled business gives no thought to selling at a panic - type valuation. * * * * * * * * * * * * at this point, a report card from me is appropriate : in 58 years of berkshire management, most of my capital - allocation decisions have been no better than so - so. in some cases, also, bad\n",
      "\n",
      "\n",
      "berkshire hathaway inc. to the shareholders of berkshire hathaway inc. : charlie munger, my long - time partner, and i have the job of managing the savings of a great number of individuals. we are grateful for their enduring trust, a relationship that often spans much of their adult lifetime. it is those dedicated savers that are forefront in my mind as i write this letter. a common belief is that people choose to save when young, expecting thereby to maintain their living standards after retirement. any assets that remain at death, this theory says, will usually be left to their families or, possibly, to friends and philanthropy. our experience has differed. we believe berkshire ’ s individual holders largely to be of the once - a - saver, always - a - saver variety. though these people live well, they eventually dispense most of their funds to philanthropic organizations. these, in turn, redistribute the funds by\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the key lessons learnt?\"\n",
    "\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "for document in retrieved_documents:\n",
    "    print(document)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c815c",
   "metadata": {},
   "source": [
    "## Using a local quantized LLM to fit into GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1e984",
   "metadata": {},
   "source": [
    "### To Do:\n",
    "\n",
    "- How to add context into prompt\n",
    "- What is the most efficient way to setup the LLM\n",
    "- Change the prompt into a parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17684b",
   "metadata": {},
   "source": [
    "### Resources on llama 2 models prompting\n",
    "\n",
    "- https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "becd3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "501fdd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mistral-7b-instruct-v0.2.Q4_K_M.gguf']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../../large/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de3fe8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Llama?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dac77842",
   "metadata": {},
   "outputs": [],
   "source": [
    "Llama.create_chat_completion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a65ba6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ../../large/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    72.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=\"../../large/mistral-7b-instruct-v0.2.Q4_K_M.gguf\", \n",
    "            n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "            chat_format=\"llama-2\")  # Set chat_format according to the model you are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38e16924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   17094.83 ms\n",
      "llama_print_timings:      sample time =      27.32 ms /   206 runs   (    0.13 ms per token,  7539.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17094.39 ms /   306 tokens (   55.86 ms per token,    17.90 tokens per second)\n",
      "llama_print_timings:        eval time =   27521.20 ms /   205 runs   (  134.25 ms per token,     7.45 tokens per second)\n",
      "llama_print_timings:       total time =   45125.79 ms /   511 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-7c48cf0e-4da7-442b-a834-4a124e037e1f',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1709046597,\n",
       " 'model': '../../large/mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \" 1. Patience is a valuable skill: Learning patience can help you focus on one thing for a long time and reap greater rewards in the future.\\n2. Learning from the past: We can gain valuable insights from studying the lives of people, both those we admire and those we don't.\\n3. Staying the course: Don't abandon a sinking ship too soon; instead, look for a seaworthy alternative.\\n4. Investing in great companies: A great company continues to thrive even after its founders or investors have moved on.\\n5. Long-term focus: Warren Buffett and Charlie Munger don't focus on short-term market fluctuations but rather seek out long-term investments.\\n6. Market volatility: The stock market behaves differently in the short term versus the long term; the long-term focus is on the intrinsic value of a company.\\n7. No guarantees in investing: There is no\"},\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 306,\n",
       "  'completion_tokens': 206,\n",
       "  'total_tokens': 512}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = \"What are the key lessons learnt?\" + \"Context: \" + results['documents'][0][0]\n",
    "llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {    \"role\": \"system\", \n",
    "             \"content\": \"Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6877f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     940.82 ms\n",
      "llama_print_timings:      sample time =      28.21 ms /   214 runs   (    0.13 ms per token,  7587.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     940.73 ms /    13 tokens (   72.36 ms per token,    13.82 tokens per second)\n",
      "llama_print_timings:        eval time =   29923.89 ms /   213 runs   (  140.49 ms per token,     7.12 tokens per second)\n",
      "llama_print_timings:       total time =   31411.09 ms /   226 tokens\n"
     ]
    }
   ],
   "source": [
    "# Simple inference example\n",
    "output = llm(\n",
    "  \"<s>[INST] {prompt} [/INST]\", # Prompt\n",
    "  max_tokens=512,  # Generate up to 512 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
